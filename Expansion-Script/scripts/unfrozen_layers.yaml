unfrozen_parameters:
- ^lm_head.weight$
- ^model.embed_tokens.weight$
- ^model.layers.11.self_attn.q_proj.weight$
- ^model.layers.11.self_attn.k_proj.weight$
- ^model.layers.11.self_attn.v_proj.weight$
- ^model.layers.11.self_attn.o_proj.weight$
- ^model.layers.11.mlp.gate_proj.weight$
- ^model.layers.11.mlp.up_proj.weight$
- ^model.layers.11.mlp.down_proj.weight$
- ^model.layers.11.input_layernorm.weight$
- ^model.layers.11.post_attention_layernorm.weight$
- ^model.layers.23.self_attn.q_proj.weight$
- ^model.layers.23.self_attn.k_proj.weight$
- ^model.layers.23.self_attn.v_proj.weight$
- ^model.layers.23.self_attn.o_proj.weight$
- ^model.layers.23.mlp.gate_proj.weight$
- ^model.layers.23.mlp.up_proj.weight$
- ^model.layers.23.mlp.down_proj.weight$
- ^model.layers.23.input_layernorm.weight$
- ^model.layers.23.post_attention_layernorm.weight$
- ^model.layers.35.self_attn.q_proj.weight$
- ^model.layers.35.self_attn.k_proj.weight$
- ^model.layers.35.self_attn.v_proj.weight$
- ^model.layers.35.self_attn.o_proj.weight$
- ^model.layers.35.mlp.gate_proj.weight$
- ^model.layers.35.mlp.up_proj.weight$
- ^model.layers.35.mlp.down_proj.weight$
- ^model.layers.35.input_layernorm.weight$
- ^model.layers.35.post_attention_layernorm.weight$
- ^model.layers.47.self_attn.q_proj.weight$
- ^model.layers.47.self_attn.k_proj.weight$
- ^model.layers.47.self_attn.v_proj.weight$
- ^model.layers.47.self_attn.o_proj.weight$
- ^model.layers.47.mlp.gate_proj.weight$
- ^model.layers.47.mlp.up_proj.weight$
- ^model.layers.47.mlp.down_proj.weight$
- ^model.layers.47.input_layernorm.weight$
- ^model.layers.47.post_attention_layernorm.weight$
- ^model.layers.59.self_attn.q_proj.weight$
- ^model.layers.59.self_attn.k_proj.weight$
- ^model.layers.59.self_attn.v_proj.weight$
- ^model.layers.59.self_attn.o_proj.weight$
- ^model.layers.59.mlp.gate_proj.weight$
- ^model.layers.59.mlp.up_proj.weight$
- ^model.layers.59.mlp.down_proj.weight$
- ^model.layers.59.input_layernorm.weight$
- ^model.layers.59.post_attention_layernorm.weight$
- ^model.layers.71.self_attn.q_proj.weight$
- ^model.layers.71.self_attn.k_proj.weight$
- ^model.layers.71.self_attn.v_proj.weight$
- ^model.layers.71.self_attn.o_proj.weight$
- ^model.layers.71.mlp.gate_proj.weight$
- ^model.layers.71.mlp.up_proj.weight$
- ^model.layers.71.mlp.down_proj.weight$
- ^model.layers.71.input_layernorm.weight$
- ^model.layers.71.post_attention_layernorm.weight$
- ^model.layers.83.self_attn.q_proj.weight$
- ^model.layers.83.self_attn.k_proj.weight$
- ^model.layers.83.self_attn.v_proj.weight$
- ^model.layers.83.self_attn.o_proj.weight$
- ^model.layers.83.mlp.gate_proj.weight$
- ^model.layers.83.mlp.up_proj.weight$
- ^model.layers.83.mlp.down_proj.weight$
- ^model.layers.83.input_layernorm.weight$
- ^model.layers.83.post_attention_layernorm.weight$
- ^model.layers.95.self_attn.q_proj.weight$
- ^model.layers.95.self_attn.k_proj.weight$
- ^model.layers.95.self_attn.v_proj.weight$
- ^model.layers.95.self_attn.o_proj.weight$
- ^model.layers.95.mlp.gate_proj.weight$
- ^model.layers.95.mlp.up_proj.weight$
- ^model.layers.95.mlp.down_proj.weight$
- ^model.layers.95.input_layernorm.weight$
- ^model.layers.95.post_attention_layernorm.weight$
